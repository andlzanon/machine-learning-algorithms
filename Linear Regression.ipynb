{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extraordinary-welsh",
   "metadata": {},
   "source": [
    "# Linear Regression with PyTorch\n",
    "\n",
    "This is a simple implementation of a Linear Regression algorithm with the Sao Paulo beer consumption dataset available in https://www.kaggle.com/dongeorge/beer-consumption-sao-paulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "declared-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-establishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Temperatura Media (C)</th>\n",
       "      <th>Temperatura Minima (C)</th>\n",
       "      <th>Temperatura Maxima (C)</th>\n",
       "      <th>Precipitacao (mm)</th>\n",
       "      <th>Final de Semana</th>\n",
       "      <th>Consumo de cerveja (litros)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>27,3</td>\n",
       "      <td>23,9</td>\n",
       "      <td>32,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>27,02</td>\n",
       "      <td>24,5</td>\n",
       "      <td>33,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>24,82</td>\n",
       "      <td>22,4</td>\n",
       "      <td>29,9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>23,98</td>\n",
       "      <td>21,5</td>\n",
       "      <td>28,6</td>\n",
       "      <td>1,2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>23,82</td>\n",
       "      <td>21</td>\n",
       "      <td>28,3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Temperatura Media (C) Temperatura Minima (C)  \\\n",
       "0  2015-01-01                  27,3                   23,9   \n",
       "1  2015-01-02                 27,02                   24,5   \n",
       "2  2015-01-03                 24,82                   22,4   \n",
       "3  2015-01-04                 23,98                   21,5   \n",
       "4  2015-01-05                 23,82                     21   \n",
       "\n",
       "  Temperatura Maxima (C) Precipitacao (mm)  Final de Semana  \\\n",
       "0                   32,5                 0                0   \n",
       "1                   33,5                 0                0   \n",
       "2                   29,9                 0                1   \n",
       "3                   28,6               1,2                1   \n",
       "4                   28,3                 0                0   \n",
       "\n",
       "   Consumo de cerveja (litros)  \n",
       "0                       25.461  \n",
       "1                       28.972  \n",
       "2                       30.814  \n",
       "3                       29.799  \n",
       "4                       28.900  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dataset\n",
    "beer_ds = pd.read_csv(\"./datasets/beer_consumption.csv\")\n",
    "beer_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "productive-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperatura Media (C)</th>\n",
       "      <th>Temperatura Minima (C)</th>\n",
       "      <th>Temperatura Maxima (C)</th>\n",
       "      <th>Precipitacao (mm)</th>\n",
       "      <th>Final de Semana</th>\n",
       "      <th>Consumo de cerveja (litros)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27,3</td>\n",
       "      <td>23,9</td>\n",
       "      <td>32,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27,02</td>\n",
       "      <td>24,5</td>\n",
       "      <td>33,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24,82</td>\n",
       "      <td>22,4</td>\n",
       "      <td>29,9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23,98</td>\n",
       "      <td>21,5</td>\n",
       "      <td>28,6</td>\n",
       "      <td>1,2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23,82</td>\n",
       "      <td>21</td>\n",
       "      <td>28,3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Temperatura Media (C) Temperatura Minima (C) Temperatura Maxima (C)  \\\n",
       "0                  27,3                   23,9                   32,5   \n",
       "1                 27,02                   24,5                   33,5   \n",
       "2                 24,82                   22,4                   29,9   \n",
       "3                 23,98                   21,5                   28,6   \n",
       "4                 23,82                     21                   28,3   \n",
       "\n",
       "  Precipitacao (mm)  Final de Semana  Consumo de cerveja (litros)  \n",
       "0                 0                0                       25.461  \n",
       "1                 0                0                       28.972  \n",
       "2                 0                1                       30.814  \n",
       "3               1,2                1                       29.799  \n",
       "4                 0                0                       28.900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the day since it is already represented as \"final de semana\"\n",
    "beer_ds = beer_ds.iloc[:,1:]\n",
    "beer_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hispanic-stream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperatura Media (C)</th>\n",
       "      <th>Temperatura Minima (C)</th>\n",
       "      <th>Temperatura Maxima (C)</th>\n",
       "      <th>Precipitacao (mm)</th>\n",
       "      <th>Final de Semana</th>\n",
       "      <th>Consumo de cerveja (litros)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25,06</td>\n",
       "      <td>19,5</td>\n",
       "      <td>30,4</td>\n",
       "      <td>16,4</td>\n",
       "      <td>1</td>\n",
       "      <td>26.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21,36</td>\n",
       "      <td>18,5</td>\n",
       "      <td>27,7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16,76</td>\n",
       "      <td>11,3</td>\n",
       "      <td>26,7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17,54</td>\n",
       "      <td>15,3</td>\n",
       "      <td>20,4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21,86</td>\n",
       "      <td>19,5</td>\n",
       "      <td>25,6</td>\n",
       "      <td>3,4</td>\n",
       "      <td>0</td>\n",
       "      <td>22.741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Temperatura Media (C) Temperatura Minima (C) Temperatura Maxima (C)  \\\n",
       "0                 25,06                   19,5                   30,4   \n",
       "1                 21,36                   18,5                   27,7   \n",
       "2                 16,76                   11,3                   26,7   \n",
       "3                 17,54                   15,3                   20,4   \n",
       "4                 21,86                   19,5                   25,6   \n",
       "\n",
       "  Precipitacao (mm)  Final de Semana  Consumo de cerveja (litros)  \n",
       "0              16,4                1                       26.836  \n",
       "1                 0                0                       22.356  \n",
       "2                 0                0                       24.227  \n",
       "3                 0                0                       20.740  \n",
       "4               3,4                0                       22.741  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suffling the dataset\n",
    "beer_ds = shuffle(beer_ds, random_state=101)\n",
    "beer_ds.reset_index(drop=True, inplace=True)\n",
    "beer_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rural-substance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Temperatura Media (C)', 'Temperatura Minima (C)',\n",
       "       'Temperatura Maxima (C)', 'Precipitacao (mm)', 'Final de Semana',\n",
       "       'Consumo de cerveja (litros)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the types of the columns\n",
    "beer_ds.dtypes\n",
    "beer_ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overall-ghana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperatura Media (C)</th>\n",
       "      <th>Temperatura Minima (C)</th>\n",
       "      <th>Temperatura Maxima (C)</th>\n",
       "      <th>Precipitacao (mm)</th>\n",
       "      <th>Final de Semana</th>\n",
       "      <th>Consumo de cerveja (litros)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.06</td>\n",
       "      <td>19.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1</td>\n",
       "      <td>26.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.36</td>\n",
       "      <td>18.5</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.76</td>\n",
       "      <td>11.3</td>\n",
       "      <td>26.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.54</td>\n",
       "      <td>15.3</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.86</td>\n",
       "      <td>19.5</td>\n",
       "      <td>25.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td>22.741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperatura Media (C)  Temperatura Minima (C)  Temperatura Maxima (C)  \\\n",
       "0                  25.06                    19.5                    30.4   \n",
       "1                  21.36                    18.5                    27.7   \n",
       "2                  16.76                    11.3                    26.7   \n",
       "3                  17.54                    15.3                    20.4   \n",
       "4                  21.86                    19.5                    25.6   \n",
       "\n",
       "   Precipitacao (mm)  Final de Semana  Consumo de cerveja (litros)  \n",
       "0               16.4                1                       26.836  \n",
       "1                0.0                0                       22.356  \n",
       "2                0.0                0                       24.227  \n",
       "3                0.0                0                       20.740  \n",
       "4                3.4                0                       22.741  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the types of the columns, first to string, then change the coma to dot, and then to float \n",
    "cols = ['Temperatura Media (C)', 'Temperatura Minima (C)',\n",
    "       'Temperatura Maxima (C)', 'Precipitacao (mm)']\n",
    "for c in cols:\n",
    "    beer_ds[c] = beer_ds[c].astype(str)\n",
    "    beer_ds[c] = beer_ds[c].str.replace(',', '.')\n",
    "    beer_ds[c] = beer_ds[c].astype(float)\n",
    "    \n",
    "beer_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "certified-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model of linear regression \n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.bn_cont = nn.BatchNorm1d(in_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fatal-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(\n",
       "  (linear): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(in_features=5, out_features=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "frozen-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset split into 20% for test and 80% for training\n",
    "X = beer_ds.drop('Consumo de cerveja (litros)',axis=1).values\n",
    "y = beer_ds['Consumo de cerveja (litros)'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=30)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tamil-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the Loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operating-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss: 28.32010269\n",
      "epoch: 11  loss: 27.62041092\n",
      "epoch: 21  loss: 26.92309189\n",
      "epoch: 31  loss: 26.22858238\n",
      "epoch: 41  loss: 25.53726578\n",
      "epoch: 51  loss: 24.84948540\n",
      "epoch: 61  loss: 24.16555977\n",
      "epoch: 71  loss: 23.48581314\n",
      "epoch: 81  loss: 22.81056595\n",
      "epoch: 91  loss: 22.14016533\n",
      "epoch: 101  loss: 21.47497749\n",
      "epoch: 111  loss: 20.81538582\n",
      "epoch: 121  loss: 20.16180229\n",
      "epoch: 131  loss: 19.51465797\n",
      "epoch: 141  loss: 18.87440872\n",
      "epoch: 151  loss: 18.24152374\n",
      "epoch: 161  loss: 17.61648941\n",
      "epoch: 171  loss: 16.99979210\n",
      "epoch: 181  loss: 16.39192772\n",
      "epoch: 191  loss: 15.79337788\n",
      "epoch: 201  loss: 15.20460033\n",
      "epoch: 211  loss: 14.62602806\n",
      "epoch: 221  loss: 14.05804157\n",
      "epoch: 231  loss: 13.50096893\n",
      "epoch: 241  loss: 12.95507622\n",
      "epoch: 251  loss: 12.42056179\n",
      "epoch: 261  loss: 11.89756584\n",
      "epoch: 271  loss: 11.38618469\n",
      "epoch: 281  loss: 10.88649559\n",
      "epoch: 291  loss: 10.39859486\n",
      "epoch: 301  loss: 9.92264080\n",
      "epoch: 311  loss: 9.45889091\n",
      "epoch: 321  loss: 9.00774288\n",
      "epoch: 331  loss: 8.56976604\n",
      "epoch: 341  loss: 8.14571190\n",
      "epoch: 351  loss: 7.73652172\n",
      "epoch: 361  loss: 7.34330702\n",
      "epoch: 371  loss: 6.96732092\n",
      "epoch: 381  loss: 6.60992241\n",
      "epoch: 391  loss: 6.27249908\n",
      "epoch: 401  loss: 5.95640755\n",
      "epoch: 411  loss: 5.66286230\n",
      "epoch: 421  loss: 5.39285851\n",
      "epoch: 431  loss: 5.14705515\n",
      "epoch: 441  loss: 4.92569351\n",
      "epoch: 451  loss: 4.72854567\n",
      "epoch: 461  loss: 4.55489874\n",
      "epoch: 471  loss: 4.40358543\n",
      "epoch: 481  loss: 4.27306747\n",
      "epoch: 491  loss: 4.16154099\n",
      "epoch: 501  loss: 4.06704950\n",
      "epoch: 511  loss: 3.98759651\n",
      "epoch: 521  loss: 3.92123723\n",
      "epoch: 531  loss: 3.86613417\n",
      "epoch: 541  loss: 3.82060361\n",
      "epoch: 551  loss: 3.78313279\n",
      "epoch: 561  loss: 3.75238466\n",
      "epoch: 571  loss: 3.72719502\n",
      "epoch: 581  loss: 3.70656443\n",
      "epoch: 591  loss: 3.68964243\n",
      "epoch: 601  loss: 3.67571521\n",
      "epoch: 611  loss: 3.66418910\n",
      "epoch: 621  loss: 3.65457535\n",
      "epoch: 631  loss: 3.64647436\n",
      "epoch: 641  loss: 3.63956285\n",
      "epoch: 651  loss: 3.63358164\n",
      "epoch: 661  loss: 3.62832379\n",
      "epoch: 671  loss: 3.62362432\n",
      "epoch: 681  loss: 3.61935472\n",
      "epoch: 691  loss: 3.61541295\n",
      "epoch: 701  loss: 3.61172056\n",
      "epoch: 711  loss: 3.60821676\n",
      "epoch: 721  loss: 3.60485411\n",
      "epoch: 731  loss: 3.60159612\n",
      "epoch: 741  loss: 3.59841585\n",
      "epoch: 751  loss: 3.59529209\n",
      "epoch: 761  loss: 3.59220934\n",
      "epoch: 771  loss: 3.58915472\n",
      "epoch: 781  loss: 3.58612084\n",
      "epoch: 791  loss: 3.58310008\n",
      "epoch: 801  loss: 3.58008742\n",
      "epoch: 811  loss: 3.57707953\n",
      "epoch: 821  loss: 3.57407379\n",
      "epoch: 831  loss: 3.57106829\n",
      "epoch: 841  loss: 3.56806159\n",
      "epoch: 851  loss: 3.56505275\n",
      "epoch: 861  loss: 3.56204128\n",
      "epoch: 871  loss: 3.55902624\n",
      "epoch: 881  loss: 3.55600786\n",
      "epoch: 891  loss: 3.55298567\n",
      "epoch: 901  loss: 3.54995918\n",
      "epoch: 911  loss: 3.54692888\n",
      "epoch: 921  loss: 3.54389429\n",
      "epoch: 931  loss: 3.54085565\n",
      "epoch: 941  loss: 3.53781271\n",
      "epoch: 951  loss: 3.53476596\n",
      "epoch: 961  loss: 3.53171515\n",
      "epoch: 971  loss: 3.52866030\n",
      "epoch: 981  loss: 3.52560163\n",
      "epoch: 991  loss: 3.52253914\n",
      "epoch: 1001  loss: 3.51947308\n",
      "epoch: 1011  loss: 3.51640296\n",
      "epoch: 1021  loss: 3.51332879\n",
      "epoch: 1031  loss: 3.51025128\n",
      "epoch: 1041  loss: 3.50717020\n",
      "epoch: 1051  loss: 3.50408506\n",
      "epoch: 1061  loss: 3.50099683\n",
      "epoch: 1071  loss: 3.49790478\n",
      "epoch: 1081  loss: 3.49480963\n",
      "epoch: 1091  loss: 3.49171042\n",
      "epoch: 1101  loss: 3.48860860\n",
      "epoch: 1111  loss: 3.48550296\n",
      "epoch: 1121  loss: 3.48239398\n",
      "epoch: 1131  loss: 3.47928238\n",
      "epoch: 1141  loss: 3.47616720\n",
      "epoch: 1151  loss: 3.47304869\n",
      "epoch: 1161  loss: 3.46992683\n",
      "epoch: 1171  loss: 3.46680236\n",
      "epoch: 1181  loss: 3.46367455\n",
      "epoch: 1191  loss: 3.46054411\n",
      "epoch: 1201  loss: 3.45741057\n",
      "epoch: 1211  loss: 3.45427394\n",
      "epoch: 1221  loss: 3.45113420\n",
      "epoch: 1231  loss: 3.44799185\n",
      "epoch: 1241  loss: 3.44484639\n",
      "epoch: 1251  loss: 3.44169855\n",
      "epoch: 1261  loss: 3.43854737\n",
      "epoch: 1271  loss: 3.43539357\n",
      "epoch: 1281  loss: 3.43223763\n",
      "epoch: 1291  loss: 3.42907858\n",
      "epoch: 1301  loss: 3.42591667\n",
      "epoch: 1311  loss: 3.42275262\n",
      "epoch: 1321  loss: 3.41958570\n",
      "epoch: 1331  loss: 3.41641617\n",
      "epoch: 1341  loss: 3.41324425\n",
      "epoch: 1351  loss: 3.41006994\n",
      "epoch: 1361  loss: 3.40689278\n",
      "epoch: 1371  loss: 3.40371370\n",
      "epoch: 1381  loss: 3.40053177\n",
      "epoch: 1391  loss: 3.39734745\n",
      "epoch: 1401  loss: 3.39416122\n",
      "epoch: 1411  loss: 3.39097238\n",
      "epoch: 1421  loss: 3.38778162\n",
      "epoch: 1431  loss: 3.38458824\n",
      "epoch: 1441  loss: 3.38139272\n",
      "epoch: 1451  loss: 3.37819505\n",
      "epoch: 1461  loss: 3.37499499\n",
      "epoch: 1471  loss: 3.37179327\n",
      "epoch: 1481  loss: 3.36858892\n",
      "epoch: 1491  loss: 3.36538315\n",
      "epoch: 1501  loss: 3.36217451\n",
      "epoch: 1511  loss: 3.35896444\n",
      "epoch: 1521  loss: 3.35575223\n",
      "epoch: 1531  loss: 3.35253811\n",
      "epoch: 1541  loss: 3.34932208\n",
      "epoch: 1551  loss: 3.34610438\n",
      "epoch: 1561  loss: 3.34288406\n",
      "epoch: 1571  loss: 3.33966255\n",
      "epoch: 1581  loss: 3.33643913\n",
      "epoch: 1591  loss: 3.33321404\n",
      "epoch: 1601  loss: 3.32998681\n",
      "epoch: 1611  loss: 3.32675791\n",
      "epoch: 1621  loss: 3.32352757\n",
      "epoch: 1631  loss: 3.32029581\n",
      "epoch: 1641  loss: 3.31706166\n",
      "epoch: 1651  loss: 3.31382632\n",
      "epoch: 1661  loss: 3.31058979\n",
      "epoch: 1671  loss: 3.30735111\n",
      "epoch: 1681  loss: 3.30411124\n",
      "epoch: 1691  loss: 3.30087018\n",
      "epoch: 1701  loss: 3.29762721\n",
      "epoch: 1711  loss: 3.29438281\n",
      "epoch: 1721  loss: 3.29113722\n",
      "epoch: 1731  loss: 3.28789067\n",
      "epoch: 1741  loss: 3.28464198\n",
      "epoch: 1751  loss: 3.28139257\n",
      "epoch: 1761  loss: 3.27814198\n",
      "epoch: 1771  loss: 3.27488971\n",
      "epoch: 1781  loss: 3.27163649\n",
      "epoch: 1791  loss: 3.26838231\n",
      "epoch: 1801  loss: 3.26512671\n",
      "epoch: 1811  loss: 3.26187038\n",
      "epoch: 1821  loss: 3.25861263\n",
      "epoch: 1831  loss: 3.25535393\n",
      "epoch: 1841  loss: 3.25209427\n",
      "epoch: 1851  loss: 3.24883413\n",
      "epoch: 1861  loss: 3.24557233\n",
      "epoch: 1871  loss: 3.24231052\n",
      "epoch: 1881  loss: 3.23904729\n",
      "epoch: 1891  loss: 3.23578358\n",
      "epoch: 1901  loss: 3.23251867\n",
      "epoch: 1911  loss: 3.22925353\n",
      "epoch: 1921  loss: 3.22598720\n",
      "epoch: 1931  loss: 3.22272062\n",
      "epoch: 1941  loss: 3.21945333\n",
      "epoch: 1951  loss: 3.21618533\n",
      "epoch: 1961  loss: 3.21291709\n",
      "epoch: 1971  loss: 3.20964813\n",
      "epoch: 1981  loss: 3.20637894\n",
      "epoch: 1991  loss: 3.20310903\n",
      "epoch: 2001  loss: 3.19983912\n",
      "epoch: 2011  loss: 3.19656873\n",
      "epoch: 2021  loss: 3.19329810\n",
      "epoch: 2031  loss: 3.19002676\n",
      "epoch: 2041  loss: 3.18675590\n",
      "epoch: 2051  loss: 3.18348479\n",
      "epoch: 2061  loss: 3.18021345\n",
      "epoch: 2071  loss: 3.17694187\n",
      "epoch: 2081  loss: 3.17367029\n",
      "epoch: 2091  loss: 3.17039847\n",
      "epoch: 2101  loss: 3.16712713\n",
      "epoch: 2111  loss: 3.16385579\n",
      "epoch: 2121  loss: 3.16058493\n",
      "epoch: 2131  loss: 3.15731406\n",
      "epoch: 2141  loss: 3.15404320\n",
      "epoch: 2151  loss: 3.15077257\n",
      "epoch: 2161  loss: 3.14750242\n",
      "epoch: 2171  loss: 3.14423275\n",
      "epoch: 2181  loss: 3.14096355\n",
      "epoch: 2191  loss: 3.13769484\n",
      "epoch: 2201  loss: 3.13442659\n",
      "epoch: 2211  loss: 3.13115883\n",
      "epoch: 2221  loss: 3.12789178\n",
      "epoch: 2231  loss: 3.12462544\n",
      "epoch: 2241  loss: 3.12136006\n",
      "epoch: 2251  loss: 3.11809516\n",
      "epoch: 2261  loss: 3.11483121\n",
      "epoch: 2271  loss: 3.11156797\n",
      "epoch: 2281  loss: 3.10830569\n",
      "epoch: 2291  loss: 3.10504460\n",
      "epoch: 2301  loss: 3.10178471\n",
      "epoch: 2311  loss: 3.09852576\n",
      "epoch: 2321  loss: 3.09526801\n",
      "epoch: 2331  loss: 3.09201121\n",
      "epoch: 2341  loss: 3.08875608\n",
      "epoch: 2351  loss: 3.08550191\n",
      "epoch: 2361  loss: 3.08224940\n",
      "epoch: 2371  loss: 3.07899833\n",
      "epoch: 2381  loss: 3.07574844\n",
      "epoch: 2391  loss: 3.07250047\n",
      "epoch: 2401  loss: 3.06925392\n",
      "epoch: 2411  loss: 3.06600928\n",
      "epoch: 2421  loss: 3.06276631\n",
      "epoch: 2431  loss: 3.05952501\n",
      "epoch: 2441  loss: 3.05628562\n",
      "epoch: 2451  loss: 3.05304790\n",
      "epoch: 2461  loss: 3.04981232\n",
      "epoch: 2471  loss: 3.04657912\n",
      "epoch: 2481  loss: 3.04334760\n",
      "epoch: 2491  loss: 3.04011869\n",
      "epoch: 2501  loss: 3.03689146\n",
      "epoch: 2511  loss: 3.03366709\n",
      "epoch: 2521  loss: 3.03044462\n",
      "epoch: 2531  loss: 3.02722478\n",
      "epoch: 2541  loss: 3.02400732\n",
      "epoch: 2551  loss: 3.02079248\n",
      "epoch: 2561  loss: 3.01758027\n",
      "epoch: 2571  loss: 3.01437092\n",
      "epoch: 2581  loss: 3.01116395\n",
      "epoch: 2591  loss: 3.00796008\n",
      "epoch: 2601  loss: 3.00475907\n",
      "epoch: 2611  loss: 3.00156116\n",
      "epoch: 2621  loss: 2.99836588\n",
      "epoch: 2631  loss: 2.99517393\n",
      "epoch: 2641  loss: 2.99198532\n",
      "epoch: 2651  loss: 2.98879981\n",
      "epoch: 2661  loss: 2.98561740\n",
      "epoch: 2671  loss: 2.98243856\n",
      "epoch: 2681  loss: 2.97926307\n",
      "epoch: 2691  loss: 2.97609138\n",
      "epoch: 2701  loss: 2.97292304\n",
      "epoch: 2711  loss: 2.96975827\n",
      "epoch: 2721  loss: 2.96659732\n",
      "epoch: 2731  loss: 2.96343994\n",
      "epoch: 2741  loss: 2.96028686\n",
      "epoch: 2751  loss: 2.95713758\n",
      "epoch: 2761  loss: 2.95399213\n",
      "epoch: 2771  loss: 2.95085120\n",
      "epoch: 2781  loss: 2.94771433\n",
      "epoch: 2791  loss: 2.94458151\n",
      "epoch: 2801  loss: 2.94145322\n",
      "epoch: 2811  loss: 2.93832922\n",
      "epoch: 2821  loss: 2.93520975\n",
      "epoch: 2831  loss: 2.93209481\n",
      "epoch: 2841  loss: 2.92898440\n",
      "epoch: 2851  loss: 2.92587876\n",
      "epoch: 2861  loss: 2.92277813\n",
      "epoch: 2871  loss: 2.91968203\n",
      "epoch: 2881  loss: 2.91659117\n",
      "epoch: 2891  loss: 2.91350508\n",
      "epoch: 2901  loss: 2.91042423\n",
      "epoch: 2911  loss: 2.90734863\n",
      "epoch: 2921  loss: 2.90427804\n",
      "epoch: 2931  loss: 2.90121293\n",
      "epoch: 2941  loss: 2.89815331\n",
      "epoch: 2951  loss: 2.89509916\n",
      "epoch: 2961  loss: 2.89205050\n",
      "epoch: 2971  loss: 2.88900757\n",
      "epoch: 2981  loss: 2.88597012\n",
      "epoch: 2991  loss: 2.88293862\n",
      "epoch: 3001  loss: 2.87991309\n",
      "epoch: 3011  loss: 2.87689352\n",
      "epoch: 3021  loss: 2.87387991\n",
      "epoch: 3031  loss: 2.87087226\n",
      "epoch: 3041  loss: 2.86787105\n",
      "epoch: 3051  loss: 2.86487603\n",
      "epoch: 3061  loss: 2.86188745\n",
      "epoch: 3071  loss: 2.85890508\n",
      "epoch: 3081  loss: 2.85592961\n",
      "epoch: 3091  loss: 2.85296035\n",
      "epoch: 3101  loss: 2.84999800\n",
      "epoch: 3111  loss: 2.84704256\n",
      "epoch: 3121  loss: 2.84409380\n",
      "epoch: 3131  loss: 2.84115171\n",
      "epoch: 3141  loss: 2.83821702\n",
      "epoch: 3151  loss: 2.83528924\n",
      "epoch: 3161  loss: 2.83236837\n",
      "epoch: 3171  loss: 2.82945514\n",
      "epoch: 3181  loss: 2.82654905\n",
      "epoch: 3191  loss: 2.82365012\n",
      "epoch: 3201  loss: 2.82075906\n",
      "epoch: 3211  loss: 2.81787539\n",
      "epoch: 3221  loss: 2.81499958\n",
      "epoch: 3231  loss: 2.81213117\n",
      "epoch: 3241  loss: 2.80927038\n",
      "epoch: 3251  loss: 2.80641770\n",
      "epoch: 3261  loss: 2.80357289\n",
      "epoch: 3271  loss: 2.80073619\n",
      "epoch: 3281  loss: 2.79790759\n",
      "epoch: 3291  loss: 2.79508710\n",
      "epoch: 3301  loss: 2.79227471\n",
      "epoch: 3311  loss: 2.78947091\n",
      "epoch: 3321  loss: 2.78667521\n",
      "epoch: 3331  loss: 2.78388810\n",
      "epoch: 3341  loss: 2.78110957\n",
      "epoch: 3351  loss: 2.77833962\n",
      "epoch: 3361  loss: 2.77557850\n",
      "epoch: 3371  loss: 2.77282596\n",
      "epoch: 3381  loss: 2.77008224\n",
      "epoch: 3391  loss: 2.76734757\n",
      "epoch: 3401  loss: 2.76462173\n",
      "epoch: 3411  loss: 2.76190495\n",
      "epoch: 3421  loss: 2.75919747\n",
      "epoch: 3431  loss: 2.75649905\n",
      "epoch: 3441  loss: 2.75380969\n",
      "epoch: 3451  loss: 2.75112987\n",
      "epoch: 3461  loss: 2.74845958\n",
      "epoch: 3471  loss: 2.74579835\n",
      "epoch: 3481  loss: 2.74314713\n",
      "epoch: 3491  loss: 2.74050522\n",
      "epoch: 3501  loss: 2.73787284\n",
      "epoch: 3511  loss: 2.73525023\n",
      "epoch: 3521  loss: 2.73263764\n",
      "epoch: 3531  loss: 2.73003483\n",
      "epoch: 3541  loss: 2.72744179\n",
      "epoch: 3551  loss: 2.72485876\n",
      "epoch: 3561  loss: 2.72228599\n",
      "epoch: 3571  loss: 2.71972299\n",
      "epoch: 3581  loss: 2.71717024\n",
      "epoch: 3591  loss: 2.71462774\n",
      "epoch: 3601  loss: 2.71209550\n",
      "epoch: 3611  loss: 2.70957375\n",
      "epoch: 3621  loss: 2.70706201\n",
      "epoch: 3631  loss: 2.70456123\n",
      "epoch: 3641  loss: 2.70207047\n",
      "epoch: 3651  loss: 2.69959044\n",
      "epoch: 3661  loss: 2.69712090\n",
      "epoch: 3671  loss: 2.69466209\n",
      "epoch: 3681  loss: 2.69221401\n",
      "epoch: 3691  loss: 2.68977666\n",
      "epoch: 3701  loss: 2.68735003\n",
      "epoch: 3711  loss: 2.68493414\n",
      "epoch: 3721  loss: 2.68252945\n",
      "epoch: 3731  loss: 2.68013549\n",
      "epoch: 3741  loss: 2.67775273\n",
      "epoch: 3751  loss: 2.67538047\n",
      "epoch: 3761  loss: 2.67301989\n",
      "epoch: 3771  loss: 2.67067003\n",
      "epoch: 3781  loss: 2.66833138\n",
      "epoch: 3791  loss: 2.66600418\n",
      "epoch: 3801  loss: 2.66368794\n",
      "epoch: 3811  loss: 2.66138291\n",
      "epoch: 3821  loss: 2.65908933\n",
      "epoch: 3831  loss: 2.65680671\n",
      "epoch: 3841  loss: 2.65453577\n",
      "epoch: 3851  loss: 2.65227628\n",
      "epoch: 3861  loss: 2.65002799\n",
      "epoch: 3871  loss: 2.64779162\n",
      "epoch: 3881  loss: 2.64556623\n",
      "epoch: 3891  loss: 2.64335227\n",
      "epoch: 3901  loss: 2.64115024\n",
      "epoch: 3911  loss: 2.63895965\n",
      "epoch: 3921  loss: 2.63678050\n",
      "epoch: 3931  loss: 2.63461304\n",
      "epoch: 3941  loss: 2.63245726\n",
      "epoch: 3951  loss: 2.63031292\n",
      "epoch: 3961  loss: 2.62818050\n",
      "epoch: 3971  loss: 2.62605977\n",
      "epoch: 3981  loss: 2.62395048\n",
      "epoch: 3991  loss: 2.62185287\n",
      "epoch: 4001  loss: 2.61976719\n",
      "epoch: 4011  loss: 2.61769342\n",
      "epoch: 4021  loss: 2.61563110\n",
      "epoch: 4031  loss: 2.61358047\n",
      "epoch: 4041  loss: 2.61154199\n",
      "epoch: 4051  loss: 2.60951495\n",
      "epoch: 4061  loss: 2.60749984\n",
      "epoch: 4071  loss: 2.60549641\n",
      "epoch: 4081  loss: 2.60350490\n",
      "epoch: 4091  loss: 2.60152483\n",
      "epoch: 4101  loss: 2.59955692\n",
      "epoch: 4111  loss: 2.59760070\n",
      "epoch: 4121  loss: 2.59565639\n",
      "epoch: 4131  loss: 2.59372377\n",
      "epoch: 4141  loss: 2.59180260\n",
      "epoch: 4151  loss: 2.58989358\n",
      "epoch: 4161  loss: 2.58799648\n",
      "epoch: 4171  loss: 2.58611083\n",
      "epoch: 4181  loss: 2.58423686\n",
      "epoch: 4191  loss: 2.58237505\n",
      "epoch: 4201  loss: 2.58052444\n",
      "epoch: 4211  loss: 2.57868600\n",
      "epoch: 4221  loss: 2.57685900\n",
      "epoch: 4231  loss: 2.57504392\n",
      "epoch: 4241  loss: 2.57324052\n",
      "epoch: 4251  loss: 2.57144856\n",
      "epoch: 4261  loss: 2.56966853\n",
      "epoch: 4271  loss: 2.56789994\n",
      "epoch: 4281  loss: 2.56614304\n",
      "epoch: 4291  loss: 2.56439757\n",
      "epoch: 4301  loss: 2.56266379\n",
      "epoch: 4311  loss: 2.56094122\n",
      "epoch: 4321  loss: 2.55923033\n",
      "epoch: 4331  loss: 2.55753088\n",
      "epoch: 4341  loss: 2.55584288\n",
      "epoch: 4351  loss: 2.55416656\n",
      "epoch: 4361  loss: 2.55250144\n",
      "epoch: 4371  loss: 2.55084753\n",
      "epoch: 4381  loss: 2.54920506\n",
      "epoch: 4391  loss: 2.54757404\n",
      "epoch: 4401  loss: 2.54595399\n",
      "epoch: 4411  loss: 2.54434538\n",
      "epoch: 4421  loss: 2.54274774\n",
      "epoch: 4431  loss: 2.54116154\n",
      "epoch: 4441  loss: 2.53958631\n",
      "epoch: 4451  loss: 2.53802204\n",
      "epoch: 4461  loss: 2.53646898\n",
      "epoch: 4471  loss: 2.53492665\n",
      "epoch: 4481  loss: 2.53339553\n",
      "epoch: 4491  loss: 2.53187513\n",
      "epoch: 4501  loss: 2.53036571\n",
      "epoch: 4511  loss: 2.52886724\n",
      "epoch: 4521  loss: 2.52737927\n",
      "epoch: 4531  loss: 2.52590227\n",
      "epoch: 4541  loss: 2.52443576\n",
      "epoch: 4551  loss: 2.52297997\n",
      "epoch: 4561  loss: 2.52153468\n",
      "epoch: 4571  loss: 2.52010012\n",
      "epoch: 4581  loss: 2.51867604\n",
      "epoch: 4591  loss: 2.51726222\n",
      "epoch: 4601  loss: 2.51585865\n",
      "epoch: 4611  loss: 2.51446557\n",
      "epoch: 4621  loss: 2.51308274\n",
      "epoch: 4631  loss: 2.51171041\n",
      "epoch: 4641  loss: 2.51034808\n",
      "epoch: 4651  loss: 2.50899577\n",
      "epoch: 4661  loss: 2.50765324\n",
      "epoch: 4671  loss: 2.50632119\n",
      "epoch: 4681  loss: 2.50499892\n",
      "epoch: 4691  loss: 2.50368667\n",
      "epoch: 4701  loss: 2.50238395\n",
      "epoch: 4711  loss: 2.50109124\n",
      "epoch: 4721  loss: 2.49980831\n",
      "epoch: 4731  loss: 2.49853492\n",
      "epoch: 4741  loss: 2.49727130\n",
      "epoch: 4751  loss: 2.49601674\n",
      "epoch: 4761  loss: 2.49477220\n",
      "epoch: 4771  loss: 2.49353695\n",
      "epoch: 4781  loss: 2.49231100\n",
      "epoch: 4791  loss: 2.49109411\n",
      "epoch: 4801  loss: 2.48988676\n",
      "epoch: 4811  loss: 2.48868847\n",
      "epoch: 4821  loss: 2.48749948\n",
      "epoch: 4831  loss: 2.48631907\n",
      "epoch: 4841  loss: 2.48514819\n",
      "epoch: 4851  loss: 2.48398590\n",
      "epoch: 4861  loss: 2.48283243\n",
      "epoch: 4871  loss: 2.48168802\n",
      "epoch: 4881  loss: 2.48055220\n",
      "epoch: 4891  loss: 2.47942519\n",
      "epoch: 4901  loss: 2.47830677\n",
      "epoch: 4911  loss: 2.47719669\n",
      "epoch: 4921  loss: 2.47609520\n",
      "epoch: 4931  loss: 2.47500229\n",
      "epoch: 4941  loss: 2.47391748\n",
      "epoch: 4951  loss: 2.47284102\n",
      "epoch: 4961  loss: 2.47177291\n",
      "epoch: 4971  loss: 2.47071290\n",
      "epoch: 4981  loss: 2.46966076\n",
      "epoch: 4991  loss: 2.46861696\n"
     ]
    }
   ],
   "source": [
    "# set number of epochs and train model passing\n",
    "# forward the train set and comparing with real values\n",
    "epochs = 5000\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = torch.sqrt(criterion(y_pred, y_train))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i%10 == 1:\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "    \n",
    "    # compute the loss, gradients and update parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expanded-circumstances",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot errors by epochs \n",
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rural-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.60434151\n"
     ]
    }
   ],
   "source": [
    "# evaluatig the test set passing the test set into the trained model\n",
    "with torch.no_grad():\n",
    "    y_val = model.forward(X_test)\n",
    "    loss = torch.sqrt(criterion(y_val, y_test))\n",
    "print(f'{loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legendary-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTED   ACTUAL     DIFF\n",
      " 1.  23.0933  23.0550   0.0383\n",
      " 2.  26.4017  26.7930   0.3913\n",
      " 3.  26.3769  21.9550   4.4219\n",
      " 4.  24.3832  28.7420   4.3588\n",
      " 5.  20.7969  23.0650   2.2681\n",
      " 6.  25.2416  27.7130   2.4714\n",
      " 7.  24.5000  23.6280   0.8720\n",
      " 8.  29.8355  30.5050   0.6695\n",
      " 9.  27.7153  23.6140   4.1013\n",
      "10.  19.3542  20.7400   1.3858\n",
      "11.  23.0393  20.2980   2.7413\n",
      "12.  30.0145  30.1270   0.1125\n",
      "13.  23.5513  25.3430   1.7917\n",
      "14.  32.1191  30.8250   1.2941\n",
      "15.  18.2800  21.4540   3.1740\n",
      "16.  27.5484  24.9740   2.5744\n",
      "17.  25.6439  22.0010   3.6429\n",
      "18.  25.7982  27.4200   1.6218\n",
      "19.  16.6311  19.4630   2.8319\n",
      "20.  27.1654  25.4890   1.6764\n",
      "21.  23.5774  22.6200   0.9574\n",
      "22.  19.8733  21.2450   1.3717\n",
      "23.  24.1046  24.5940   0.4894\n",
      "24.  24.8517  24.2130   0.6387\n",
      "25.  26.6860  22.3890   4.2970\n",
      "26.  24.8408  24.1140   0.7268\n",
      "27.  36.8018  37.6900   0.8882\n",
      "28.  22.5866  21.5930   0.9936\n",
      "29.  36.2731  30.5240   5.7491\n",
      "30.  28.7154  26.2720   2.4434\n",
      "31.  21.0856  24.2150   3.1294\n",
      "32.  26.7290  29.0900   2.3610\n",
      "33.  24.7435  22.0390   2.7045\n",
      "34.  31.8549  29.5790   2.2759\n",
      "35.  28.1739  31.5720   3.3981\n",
      "36.  26.1977  28.7490   2.5513\n",
      "37.  25.7741  28.9790   3.2049\n",
      "38.  29.1384  27.4880   1.6504\n",
      "39.  27.2005  30.1770   2.9765\n",
      "40.  28.9590  32.7130   3.7540\n",
      "41.  29.3687  29.8290   0.4603\n",
      "42.  21.8488  22.7720   0.9232\n",
      "43.  22.7169  24.7380   2.0211\n",
      "44.  20.2232  21.0730   0.8498\n",
      "45.  23.8348  23.5370   0.2978\n",
      "46.  26.6728  23.9370   2.7358\n",
      "47.  23.3464  21.7480   1.5984\n",
      "48.  23.8317  27.5940   3.7623\n",
      "49.  31.5107  27.8710   3.6397\n",
      "50.  27.3492  30.7400   3.3908\n",
      "51.  23.1943  26.6910   3.4967\n",
      "52.  28.8732  24.2190   4.6542\n",
      "53.  19.6460  21.8250   2.1790\n",
      "54.  26.4139  24.5670   1.8469\n",
      "55.  22.3757  24.6320   2.2563\n",
      "56.  24.9751  23.4690   1.5061\n",
      "57.  19.7965  20.5380   0.7415\n",
      "58.  26.0948  27.1370   1.0422\n",
      "59.  26.5969  28.6100   2.0131\n",
      "60.  19.0898  21.2370   2.1472\n",
      "61.  24.7196  22.4090   2.3106\n",
      "62.  29.9289  32.7800   2.8511\n",
      "63.  26.3027  24.3370   1.9657\n",
      "64.  28.5015  31.1390   2.6375\n",
      "65.  20.4211  24.6790   4.2579\n",
      "66.  29.3511  25.7240   3.6271\n",
      "67.  18.3819  21.2940   2.9121\n",
      "68.  28.6375  31.3870   2.7495\n",
      "69.  22.8626  21.1040   1.7586\n",
      "70.  27.8808  32.4730   4.5922\n",
      "71.  24.2519  24.4290   0.1771\n",
      "72.  22.9773  26.5940   3.6167\n",
      "73.  18.8547  19.7270   0.8723\n"
     ]
    }
   ],
   "source": [
    "# comparing predicted, actual and the difference\n",
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(len(y_test)):\n",
    "    diff = np.abs(y_val[i].item() - y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
